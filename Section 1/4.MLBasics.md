## Machine Learning Basics

>> A computer program is said to learn from experince E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P improves with experience E

## 1.Learning Algorithms

Task T: Classification, Regression, Transcription, Machine Translation, Structured Output, Anomaly Detection, Synthesis and Sampling, Imputation of missing values, Denoising, Density Estimation

Performance P: Accuracy, Precision, Recall, AUC, F-score, RMSE ...

Experience E: Unsupervised, Supervised, Reinforcement Learning

## 2. Capacity, Overfitting, Underfitting

The central challenge in machine learning is making it **generalizable** to unseen datasets

In the data generation process, **i.i.d Assumption** assumes that the datasets are independent, and identically distributed

Underfitting occurs when the model is not able to obtain a sufficiently low error on the training set

Overfitting occurs when the difference between the training error and testing error is high

We can control a model's tendency to over/underfit by changing it's capacity.

A model's capacity is it's ability to fit a wide variety of functions.

Models with low capacity may struggle to fit the training set, while models with high capacity can overfit the training set by memorizing the properties of the training set that may not be present in the test set.

One way to control the model's capacity is to choose it's **hypothesis space**, which is the set of functions that the model is allowed to select as being the solution

>> Example: In linear regression, we can increase the capcity by allowing it to include polynomials, rather than just linear functions.

**Models perform best when their capcity approximately captures the complexity of the task they need to perform**

Other than changing the amount of input features and adding new parameters to those features, we can change the capacity by allowing the model to choose which family of functions to choose from. This is called **representational capacity**.

## 2.1 No Free Lunch Theorem

The No Free Lunch Theorem states that averaged over all possible data-generating distributions, every classification algorithm has the same error rate when classifying previously unobserved points. 

In other words, no machine learning algorithm is universally better than the other.

But fortunately, machine learning research does not seek a universial learning algorithm (yet), but is only applied to local problems.

## 2.2 Regularization

One method of modifying a learning algorthim is to increase or decrease the model's representational capacity by adding or removing functions from the hypothesis space.

Another way of modifying is to include a weight decay, or a **regularizer** as a penalty to the cost function. A larger regularizer forces the weights to become smaller.

Regularization reduces generalization error, without reducing training error.

## 3. Hyperparameters and Validation Sets

Hyperparameters are used to control the model's behavior.

If the model solely learns on the training set, the hyperparameters chosen will be those that maximize the the training score, which can result in overfitting.

One way to combat this is to have a **validation set**, which is a fraction of the training set that is not used for training, but for testing the model's performance.

No data from the test set should be used in the validation set as well. This will lead to data leakage.

## 3.1 Cross Validation

To avoid any bad splits when doing validation, we choose different segments of the training set to be the validation set, and we average the performance across each set.

K-fold cross validation chooses `K` number of non-overlapping segments from the training set.

## 4. Estimators, Bias and Variance

**Point Estimation** involves the use of sample data to calculate a single value (known as a point estimate or statistic) which is to serve as a "best guess" or "best estimate" of an unknown population parameter

Point Estimation can also refer to the relationship between input and target variables. We call that **Function Estimation**
 
The **Bias** of an estimator is the difference between the estimator's expected value and the true value of the parameter being estimated. Having a low bias is often (but not always) preferred. A low bias would mean the expected value and true value are close to each other.
 
The **Variance** of an estimator is how much the estimator varies as we randomly resample data points from the same data generator. Having low variance is preferred, as we want the estimator to have consistent results. The **Standard Error** is the square root of the Variance.
 
There is the important component of **Bias-Variance** trade off. Bias and Variance measures two different sources of errors in the estimator. Bias measures expected deviation from the true value. Variance measures measures the deviation from the expected estimator values over various data samples from the same data generator.
 
 The most common trade-off is to use cross-validation. We can also use **Mean Squared Error (MSE)**
 
 The MSE measures the overall expected deviations between the estimator and the true value of the parameter. Evaluating the MSE takes into account both bias and variance, thus when we minimize MSE, we implicitly optimize bias and variance
 
 **Strong Consistency** states that as the training set increases, the estimator converges to the true value of the parameter. Consistency ensures that the bias in the estimator diminishes as the training set grows.
 
 ## 5. Maximum Likelihood Estimation

Rather than guessing that some function might be a good estimator, then analyzing it's bias and variance, we need to have some principle that we can use to derive the estimator for different models

This principle is the Maximum Likelihood Principle.

One way to interpret the MLE is to view it as minimizing the dissimilarity between distributions (measured in terms of KL divergence) of the training set and the model prediction.

When we are maximizing the MLE, we are minimizng the KL divergence between training and model prediction, which in turn minimizes the cross-entropy between the two distributions.

MLE thus seeks to make the models prediction distribution match the training set. Ideally, we would like to match the true data distribution, but we don't have access to it.

## 5.1 Conditional Log-Likelihood and Mean Squared Error

The MLE can be generalized to estimate a conditional probability, to predict label `y` given training data `x`

Example: Linear Regression

Linear Regression takes in inputs `x` and produces an output prediction value `y'`

We can also view it as Linear Regression produces a conditional probability distribution `P(y|x)`

Thus, when we want to maximize the MLE, we are trying to make the conditional probablity distribution match the training set's distribution.

Ideally, the testing set should have the same distribution as the training set, and so the probability distribution produced by the model should be able to generalize to the test set, and make correct predictions

## 5.2 Properties of MLE

MLE has the property of consistency, meaning that as the number of training examples increases, the estimate parameter converges to the true value.

## 6. Bayesian Statistics

Previously, we have seen us trying to estimate a single parametric value, and make all predictions based on that value.

In Bayesian Statistics however, we consider all possible parametric values before making a prediction.

Bayesian Statistics observes the dataset, and makes assumptions upon it before performing predicitons. These assumptions are called the **prior**

## 6.1 Maximum A Posteriori Estimation (MAP)

While the most principled approach is to make predicitions using the full Bayesian posterior distribution, it is often desirable to have a single point estimate.

We can use the prior observation to influence the choice of the point estimate by choosing the maximum a posterior point estimate.

MAP chooses the point of maximal posterior probability.

## 7 Supervised Learning Algorithms

## 7.1 Probabilistic Supervised Learning

Most superivsed learning algorithms are based on estimating a probability distribution. This is usually done by maximizing the MLE

In Logistic Regression, we must search for optimal weights by maximizing the log-likelihood, by minimizing the negative log-likelihood using gradient descent.

## 7.2 Support Vector Machines (SVM)

This model is similar to logistic regression in that it is driven by a linear function. Unlike logistic regression however, SVM does not provide probabilities, but only class identities.

One key innovation to the SVM is the kernel trick. The kernel trick transform the linear relationship in the data to a non-linear one. This transform the data points to a higher dimension space, which may become more seperable.

The most common kernel is the Gaussian Kernel,or the Radial Basis Function (RBF) kernel.

The downside to an SVM is that the cost of predicition grows linearly to the number of training examples.

## 7.3 Other Simple Supervised Algorithms

K Nearest Neighbour (Classifies a new point based on it's nearest neighbours)

Decision Tree (Breaks data down into smaller subsets. Choice of breaking depends on it's information gain, or entropy loss)

## 8. Unsupervised Learning Algorithms

Unsupervised learning refers to extracting information without human annotation tasks.

One example is trying to find the best representation of a dataset. (Best meaning we are trying to preserve as must information as possible)

Three such representations are: Lower dimension representation, Sparse representation and Independent represenations

Low Dimension Representation attempts to compress the information

Sparse Representation embeds the dataset into a representation that is mostly zeros, but does not discard much information

Independent Representation tries to disentangle information or variations in the underlying distribution

These three representations are not mutually exclusive

## 8.1 Principle Component Analysis (PCA)

PCA can be seen as an unsupervised task to learn representations of data before compressing it.

PCA learns a lower dimension of the original data, and learns to represent elements that how no linear correlation to each other.

## 8.2 K-means clustering

Another simple representation learning algorithm is k-means clustering.

K means divides the set into k different clusters that are near each other.

K means is also an example of sparse representation, because it give a one-hot encoding for the group that the data point is in

K means initializes `x` number of clusters, and repeats two steps until convergence

1. training examples are assigned to a cluster centroid
2. a new centroid is calulated based on the new training data points added to it

Clustering may result in information loss

Example: Dataset has Red Cars and Blue Cars + Red Trucks and Blue Trucks. The clustering algorithm may cluster according to color, or to car type. Either one of the clusters will lose the other information

## 9. Stochastic Gradient Descent

Nearly all of deep learning algorithms are powered by SGD. SGD is an extension of gradient descent.

A common problem with deep learning is that large number of training samples are good for generalization, but it increases the cost of computation as well.

The cost function of an algorithm is often the sum of the loss of all training examples.

SGD thus does an approximation of the gradient by sampling in mini-batches, instead of the entire training set at once.

## 10. Building a Machine Learning Algorithm

Nearly all deep learning algorithms have the same building blocks:

1. Dataset
2. Cost function
3. Optimization function
4. Model

Most common Cost function: negative loglikelihood (how closely does the distribution of your model's output match the true distribution of the data)

## 11. Challenges in Deep Learning

1. Curse of Dimensionality

When the number of dimensions and possible configurations is much larger than the number of training examples

If the example is never seen in the training space, how can we classify it? One solution is to take the closest example and approximate to it.

2. Local Constancy and Smoothness Regularization

Most models have a prior belief of **smootheness prior** or **local constancy prior**. These priors state that the functions we learn should not change very much within a small region

These priors work well if we have enough training examples to cover all dimension, but given a high dimensionality dataset, even a smooth function can change drastically as it traverses through different dimensions.

3. Manifold Learning

A manifold is a connected region that can be approximated well using only a small subet of points

This is akin to dimensionality reduction, we where can use only a small set of features or inputs to approximate the entire dataset
