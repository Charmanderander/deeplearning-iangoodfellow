## Numerical Computation

This chapter describes the various computations for common operations in machine learning

## 1. Overflow and Underflow

Representing infinitely many real numbers on a finite number of bits in the computer can cause **Rounding Error**

**Underflow** happens when values close to `0` gets rounded to `0`, which will cause divison errors

**Overflow** happens when numbers with large magnitude are approximated to infinity (or negative infinity), which will result in NaN (Not a Number)

## 2. Poor Conditioning

Conditioning refers to how fast a function changes with respect to small changes to it's inputs.

Functions that change rapidly when their inputs are changed slightly are problematic because rounding errors in the inputs can cause large changes in the outputs.

## 3. Gradient-Based Optimization

Optimization is the process of minimizing or maximizing some function `f(x)` by changing the value of `x`

Most problems are framed as Minimization: Maximatization of `f(x)` = Minimization of `-f(x)`

The function to minimize is called the **Objective function, cost function, loss function or error function**

Deriving `f(x)` gives us `f'(x)`, which gives us the slope of `f(x)` at point `x`. Intuitively, it tells us how much `f(x)` changes when `x` changes

The derivative `f'(x)` tells us how to minimize the function `f(x)` by telling us to move in the opposite direction of the sign of `f'(x)`

The goal is to get `f'(x) = 0`, which is a minumum point.

**Saddle points** are points where `f'(x) = 0`, but are neither a minima nor a maxima

In deep learning, the problems are usually very complex such that there are many local minimas. We this settle for a very low local minima, even though it may not be the global minima

In multi-dimensional problems, where the number of inputs > 1, critical points are when every element of the gradient = 0

## 3.1 Beyond the Gradient: Jacobian and Hessian Matrices

The partial derivative of a function whose inputs and outputs are both vectors can be represented by a **Jacobian Matrix**

The Jacobian matrix is a matrix containing all the partial derivatives

A matrix of 2nd derivatives are called a **Hessian Matrix**

While 1st derivatives tells us how much `f(x)` changes wrt to `x`, the 2nd derivatives `f''(x)` tells us how much `f'(x)` changes wrt to x

Intuitively, the 2nd derivative tells us the curvature of the slope. 

`f''(x) == 0` : It is a straight lined slope. When `x` changes by `i`, `f'(x)` changes by `i`. Inconclusive minima/maxima

`f''(x) > 0` : It is an upward curved slope. When `x` changes by `i`, `f'(x)` changes < `i`. This is a minima

`f''(x) < 0` : It is a downward curved slope. When `x` changes by `i`, `f'(x)` changes > `i`. This is a maxima

Saddle points can be found by observing the eigenvalues of a Hessian Matrix when `f'(x) = 0`

If all eigenvalues are positive, the point is at a local minimum.

If all eigenvalues are negative, the point is at a local maximum.

If at least one eigenvalue is positive, and one eigenvalue is negative, it is at a saddle point (It's at a minima for one dimension, at a maxima for another dimension)

The test is inconclusive if all eigenvalues have the same sign, with at least one eigenvalue = 0

